# Задание 3: Векторный индекс базы знаний

## Описание

Данный проект создает векторный индекс для базы знаний Star Wars, используя модель эмбеддингов BGE-base-en-v1.5 и векторную базу данных ChromaDB.

## Используемые технологии

### Модель эмбеддингов
- **Название:** BGE-base-en-v1.5
- **Репозиторий:** https://huggingface.co/BAAI/bge-base-en-v1.5
- **Размер эмбеддингов:** 768 измерений
- **Особенности:** Оптимизирована для английского языка, показывает высокое качество поиска (84.7% top-5 точность)

### Векторная база данных
- **Название:** ChromaDB
- **Версия:** 0.4.0+
- **Тип:** PersistentClient (локальное хранение)
- **Алгоритм поиска:** HNSW (Hierarchical Navigable Small World)

### База знаний
- **Источник:** Документы из Task2/knowledge_base/
- **Количество документов:** 40 текстовых файлов о персонажах, планетах, технологиях и концепциях Star Wars
- **Формат:** Очищенные текстовые файлы с замененными терминами

## Краткое описание результатов

После выполнения `build_index.py` в файле `index_stats.json` сохраняется следующая информация:

- **Модель:** BGE-base-en-v1.5 (BAAI/bge-base-en-v1.5)
- **База знаний:** 40 документов из Task2/knowledge_base/
- **Количество чанков:** Зависит от размера документов (обычно 1000-1500 чанков)
- **Время генерации:** Зависит от системы (обычно 5-15 минут на CPU, 2-5 минут на GPU)

**Примечание:** Точные значения будут указаны в файле `index_stats.json` после выполнения индексации.

## Параметры индексации

- **Размер чанка:** 1000 символов (~500-1000 токенов)
- **Перекрытие чанков:** 200 символов (для сохранения контекста)
- **Метод разбиения:** RecursiveCharacterTextSplitter из LangChain
- **Разделители:** Абзацы → Предложения → Слова

## Структура проекта

```
Task3/
├── build_index.py          # Скрипт для создания индекса
├── test_search.py          # Скрипт для тестирования поиска
├── requirements.txt        # Зависимости проекта
├── README.md              # Данная инструкция
├── chroma_db/             # Папка с векторной базой данных (создается автоматически)
└── index_stats.json       # Статистика индексации (создается автоматически)
```

## Установка зависимостей

1. Перейдите в директорию Task3:
```bash
cd Task3
```

2. Установите зависимости:
```bash
pip install -r requirements.txt
```

**Примечание:** При первом запуске модель BGE-base-en-v1.5 будет автоматически загружена с Hugging Face (размер ~420 МБ). Убедитесь, что у вас есть доступ к интернету и достаточно свободного места на диске.

## Подробная инструкция по запуску и проверке

### Шаг 1: Подготовка окружения

1. **Проверьте наличие базы знаний:**
   - Убедитесь, что папка `../Task2/knowledge_base/` существует
   - В ней должны быть текстовые файлы (например, `Luke_Skywalker.txt`, `The_Force.txt` и т.д.)

2. **Проверьте Python версию:**
   ```bash
   python --version
   ```
   Рекомендуется Python 3.8 или выше.

3. **Установите зависимости:**
   ```bash
   python -m pip install --upgrade pip
   pip install -r requirements.txt
   ```
   
   **Важно:** Установка может занять 5-10 минут. Дождитесь сообщения "Successfully installed ..."
   
4. **Проверьте установку:**
   ```bash
   python -c "import sentence_transformers; print('OK')"
   python -c "import chromadb; print('OK')"
   ```

### Шаг 2: Создание векторного индекса

**Команда:**
```bash
python build_index.py
```

**Что происходит во время выполнения:**

1. **Загрузка документов** (Шаг 1)
   - Скрипт сканирует папку `Task2/knowledge_base/`
   - Загружает все `.txt` файлы
   - Выводит количество найденных документов

2. **Разбиение на чанки** (Шаг 2)
   - Каждый документ разбивается на логически связанные фрагменты
   - Размер чанка: 1000 символов
   - Перекрытие: 200 символов (для сохранения контекста)
   - Выводит общее количество созданных чанков

3. **Загрузка модели** (Шаг 3)
   - Загружается модель BGE-base-en-v1.5
   - При первом запуске модель скачивается с Hugging Face (~420 МБ)
   - Последующие запуски используют кэшированную версию

4. **Генерация эмбеддингов** (Шаг 4)
   - Для каждого чанка создается векторное представление (768 измерений)
   - Процесс отображается прогресс-баром
   - Время зависит от количества чанков и наличия GPU

5. **Создание индекса** (Шаг 5)
   - Создается база данных ChromaDB в папке `chroma_db/`
   - Все чанки с эмбеддингами и метаданными сохраняются в индекс
   - Выводится подтверждение успешного создания

**Ожидаемый результат:**
- Создается папка `chroma_db/` с векторной базой данных
- Создается файл `index_stats.json` со статистикой индексации
- В консоли выводится информация о количестве документов, чанков и времени выполнения

**Пример вывода:**
```
============================================================
Создание векторного индекса базы знаний
============================================================
Модель эмбеддингов: BAAI/bge-base-en-v1.5
Размер эмбеддингов: 768
Размер чанка: 1000 символов
Перекрытие чанков: 200 символов

Шаг 1: Загрузка документов из базы знаний...
Найдено 40 документов для обработки
Загружено документов: 40

Шаг 2: Разбиение документов на чанки...
Создано чанков: 1250

Шаг 3: Загрузка модели эмбеддингов...
Модель загружена

Шаг 4: Генерация эмбеддингов...
Генерация эмбеддингов для 1250 чанков...
100%|████████████| 40/40 [02:15<00:00,  3.38s/it]
Сгенерировано эмбеддингов: 1250

Шаг 5: Создание векторного индекса в ChromaDB...
Добавление чанков в индекс...
Индекс создан: 1250 чанков добавлено

============================================================
Индексация завершена!
============================================================
Время выполнения: 135.45 секунд (2.26 минут)
Документов обработано: 40
Чанков создано: 1250
Среднее чанков на документ: 31.25
Путь к индексу: C:\...\Task3\chroma_db
```

### Шаг 3: Проверка качества индекса

**Команда:**
```bash
python test_search.py
```

**Что происходит во время выполнения:**

1. **Проверка наличия индекса**
   - Скрипт проверяет существование папки `chroma_db/`
   - Если индекс не найден, выводится сообщение об ошибке

2. **Загрузка модели и подключение к БД**
   - Загружается модель эмбеддингов
   - Подключается к ChromaDB
   - Выводится количество чанков в индексе

3. **Выполнение тестовых запросов**
   - Выполняются 5 предопределенных запросов
   - Для каждого запроса находятся топ-3 наиболее релевантных чанка
   - Выводится детальная информация о каждом результате

**Ожидаемый результат:**
- Для каждого запроса выводятся релевантные результаты
- Каждый результат содержит:
  - Название файла-источника
  - Заголовок статьи
  - Позицию чанка в документе
  - Косинусное расстояние (чем меньше, тем релевантнее)
  - Превью текста чанка (первые 500 символов)

**Пример вывода:**
```
================================================================================
Тестирование поиска в векторном индексе
================================================================================

Загрузка модели эмбеддингов...
Модель загружена

Подключение к векторной базе данных...
Коллекция 'star_wars_knowledge_base' загружена
Всего чанков в индексе: 1250

Выполнение тестовых запросов...

================================================================================
Запрос: What is the Force and how does it work?
================================================================================

Результат #1
  Файл: The_Force.txt
  Заголовок: The Force
  Чанк: 1/45
  Расстояние (косинусное): 0.3245
  Текст чанка:
  ----------------------------------------------------------------------------
  The Force was a metaphysical and ubiquitous power in the Star Wars universe...
```

### Шаг 4: Проверка статистики

После выполнения `build_index.py` создается файл `index_stats.json`:

```bash
cat index_stats.json
```

**Содержимое файла:**
```json
{
  "model": "BAAI/bge-base-en-v1.5",
  "embedding_size": 768,
  "chunk_size": 1000,
  "chunk_overlap": 200,
  "documents_count": 40,
  "chunks_count": 1250,
  "indexing_time_seconds": 135.45,
  "index_path": "C:\\...\\Task3\\chroma_db"
}
```

## Запуск (краткая версия)

**Что происходит:**
1. Загружаются все документы из `../Task2/knowledge_base/`
2. Документы разбиваются на логически связанные чанки
3. Для каждого чанка генерируется векторное представление (эмбеддинг)
4. Эмбеддинги сохраняются в ChromaDB с метаданными

**Время выполнения:** Зависит от количества документов и производительности системы (обычно 5-15 минут)

**Результат:**
- Создается папка `chroma_db/` с векторным индексом
- Создается файл `index_stats.json` со статистикой

### 2. Тестирование поиска

```bash
python test_search.py
```

**Что происходит:**
1. Загружается созданный индекс из ChromaDB
2. Выполняются тестовые запросы
3. Для каждого запроса выводятся топ-3 наиболее релевантных чанка

**Примеры запросов:**
- "What is the Force and how does it work?"
- "Tell me about Luke Skywalker's training"
- "What is a lightsaber and how is it constructed?"
- "Who is Darth Vader and what is his story?"
- "What is the Death Star?"

## Метаданные чанков

Каждый чанк в индексе содержит следующие метаданные:

- **source:** Полный путь к исходному файлу
- **filename:** Имя файла
- **title:** Название статьи (извлечено из имени файла)
- **chunk_id:** Уникальный идентификатор чанка
- **chunk_index:** Номер чанка в документе (начиная с 0)
- **total_chunks:** Общее количество чанков в документе

Эти метаданные позволяют:
- Отслеживать источник информации для цитирования
- Восстанавливать контекст документа
- Фильтровать результаты по источникам

## Статистика индексации

После выполнения `build_index.py` создается файл `index_stats.json` со следующей информацией:

- Модель эмбеддингов
- Размер эмбеддингов
- Параметры разбиения на чанки
- Количество обработанных документов
- Количество созданных чанков
- Время индексации

## Проверка качества

Скрипт `test_search.py` автоматически проверяет качество поиска на нескольких примерах запросов. Для каждого запроса выводятся:

1. **Топ-3 наиболее релевантных чанка**
2. **Метаданные:** файл, заголовок, позиция чанка
3. **Расстояние:** косинусное расстояние (меньше = более релевантно)
4. **Превью текста:** первые 500 символов чанка

**Критерии качества:**
- Найденные чанки должны быть семантически связаны с запросом
- Метаданные должны корректно указывать на источник
- Расстояния должны быть разумными (обычно < 0.5 для релевантных результатов)

## Использование индекса в коде

Пример использования индекса в вашем приложении:

```python
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer

# Загрузка модели
model = SentenceTransformer("BAAI/bge-base-en-v1.5")

# Подключение к базе
client = chromadb.PersistentClient(path="chroma_db")
collection = client.get_collection("star_wars_knowledge_base")

# Поиск
query = "What is the Force?"
query_embedding = model.encode(query, convert_to_numpy=False).tolist()

results = collection.query(
    query_embeddings=query_embedding,
    n_results=5
)

# Обработка результатов
for doc, metadata in zip(results["documents"][0], results["metadatas"][0]):
    print(f"Источник: {metadata['title']}")
    print(f"Текст: {doc[:200]}...")
```

## Устранение неполадок

### Ошибка: "Индекс не найден"
- Убедитесь, что вы запустили `build_index.py` перед использованием поиска
- Проверьте, что папка `chroma_db/` существует

### Ошибка: "Модель не найдена"
- При первом запуске модель загружается автоматически
- Убедитесь, что у вас есть доступ к интернету
- Проверьте наличие свободного места на диске (~500 МБ)

### Ошибка: "ModuleNotFoundError: No module named 'sentence_transformers'"

**Быстрая диагностика:**
```bash
python check_installation.py
```

Этот скрипт проверит все необходимые модули и покажет, что именно не установлено.

**Причины и решения:**

1. **Установка не завершилась (наиболее частая причина):**
   - Установка больших пакетов (torch, transformers) может занять 5-10 минут
   - Дождитесь сообщения "Successfully installed ..." в конце
   - Если установка прервалась или зависла:
     ```bash
     # Прервите установку (Ctrl+C) и попробуйте снова:
     pip install -r requirements.txt --no-cache-dir
     ```
   - Или установите критичные пакеты по отдельности:
     ```bash
     pip install sentence-transformers
     pip install chromadb
     pip install langchain langchain-core langchain-community
     ```

2. **Установка в другое окружение Python:**
   - Проверьте, какой Python используется:
     ```bash
     python --version
     where python  # Windows
     which python  # Linux/Mac
     ```
   - Убедитесь, что используете тот же Python для установки и запуска
   - **Рекомендуется использовать виртуальное окружение:**
     ```bash
     python -m venv venv
     venv\Scripts\activate  # Windows
     source venv/bin/activate  # Linux/Mac
     pip install -r requirements.txt
     ```

3. **Проверка установленных пакетов:**
   ```bash
   pip list | findstr sentence-transformers  # Windows
   pip list | grep sentence-transformers      # Linux/Mac
   pip show sentence-transformers
   ```

4. **Полная переустановка (если ничего не помогает):**
   ```bash
   pip uninstall sentence-transformers chromadb langchain langchain-core langchain-community -y
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

5. **Альтернатива: установка минимальных версий:**
   Если проблемы продолжаются, попробуйте установить конкретные версии:
   ```bash
   pip install sentence-transformers==2.2.2
   pip install chromadb==0.4.22
   pip install langchain==0.1.0 langchain-core==0.1.0 langchain-community==0.0.20
   ```

### Медленная индексация
- Используйте GPU для ускорения (автоматически, если доступно)
- Уменьшите размер чанков, если память ограничена
- Используйте меньшую модель (например, all-MiniLM-L6-v2)

### Низкое качество поиска
- Увеличьте размер чанков (до 1500-2000 символов)
- Увеличьте перекрытие чанков (до 300-400 символов)
- Попробуйте другую модель эмбеддингов

## Дополнительная информация

- **Размер индекса:** Зависит от количества чанков (обычно 50-200 МБ для 40 документов)
- **Скорость поиска:** <100 мс для типичных запросов
- **Масштабируемость:** ChromaDB может обрабатывать миллионы векторов

## Следующие шаги

После создания индекса вы можете:
1. Интегрировать поиск в RAG-бот
2. Добавить фильтрацию по метаданным
3. Реализовать гибридный поиск (семантический + keyword)
4. Настроить ранжирование результатов

## Описание действий и архитектуры решения

### Архитектура решения

Решение состоит из трех основных компонентов:

1. **Модель эмбеддингов (BGE-base-en-v1.5)**
   - Преобразует текстовые чанки в векторные представления
   - Размерность: 768 измерений
   - Обучена на больших корпусах текстов для понимания семантики

2. **Векторная база данных (ChromaDB)**
   - Хранит эмбеддинги и метаданные
   - Использует алгоритм HNSW для быстрого поиска
   - Поддерживает персистентное хранение на диске

3. **Процесс индексации**
   - Разбиение документов на чанки для сохранения контекста
   - Генерация эмбеддингов для каждого чанка
   - Сохранение с метаданными для отслеживания источников

### Детальное описание процесса

#### 1. Загрузка документов

**Функция:** `load_documents()`

**Действия:**
- Сканирует папку `Task2/knowledge_base/` на наличие `.txt` файлов
- Для каждого файла:
  - Читает содержимое в кодировке UTF-8
  - Извлекает название статьи из имени файла (заменяет `_` на пробелы)
  - Создает объект `Document` с метаданными (source, filename, title)
- Возвращает список всех документов

**Обработка ошибок:**
- Пропускает файлы, которые не удалось прочитать
- Выводит предупреждение для проблемных файлов

#### 2. Разбиение на чанки

**Функция:** `split_documents()`

**Действия:**
- Использует `RecursiveCharacterTextSplitter` из LangChain
- Параметры:
  - `chunk_size=1000`: максимальный размер чанка в символах
  - `chunk_overlap=200`: перекрытие между соседними чанками
  - Разделители: абзацы (`\n\n`) → предложения (`\n`) → слова (` `)
- Для каждого документа:
  - Разбивает на чанки с сохранением контекста
  - Добавляет метаданные: `chunk_id`, `chunk_index`, `total_chunks`
  - Присваивает уникальный идентификатор каждому чанку

**Почему перекрытие важно:**
- Сохраняет контекст на границах чанков
- Предотвращает потерю информации при разбиении предложений
- Улучшает качество поиска для запросов на границах чанков

#### 3. Генерация эмбеддингов

**Функция:** `create_embeddings()`

**Действия:**
- Загружает модель BGE-base-en-v1.5 (кэшируется после первого использования)
- Извлекает тексты из всех чанков
- Генерирует эмбеддинги батчами по 32 чанка (для оптимизации памяти)
- Преобразует в список списков (формат для ChromaDB)

**Оптимизации:**
- Батчинг уменьшает количество вызовов модели
- Автоматическое использование GPU, если доступно
- Прогресс-бар показывает процесс генерации

#### 4. Создание индекса в ChromaDB

**Функция:** `build_index()` (основная логика)

**Действия:**
- Создает или очищает папку `chroma_db/`
- Инициализирует `PersistentClient` ChromaDB
- Создает коллекцию `star_wars_knowledge_base`
- Подготавливает данные:
  - `ids`: уникальные идентификаторы чанков (`chunk_0`, `chunk_1`, ...)
  - `embeddings`: векторные представления
  - `documents`: тексты чанков
  - `metadatas`: метаданные (source, filename, title, chunk_id, chunk_index, total_chunks)
- Добавляет все данные в коллекцию одной операцией

**Структура индекса:**
```
chroma_db/
├── chroma.sqlite3          # SQLite база для метаданных
└── [другие файлы индекса]  # Векторные данные и индексы HNSW
```

#### 5. Поиск в индексе

**Функция:** `search_query()` в `test_search.py`

**Действия:**
- Генерирует эмбеддинг для запроса пользователя
- Выполняет поиск в ChromaDB с помощью `collection.query()`
- ChromaDB использует косинусное расстояние для нахождения ближайших векторов
- Возвращает топ-K наиболее релевантных чанков

**Алгоритм поиска (HNSW):**
- Иерархическая структура графа для быстрого поиска
- Время поиска: O(log N), где N - количество векторов
- Точность: высокая (близка к точному поиску)

### Преимущества выбранного подхода

1. **Локальное развертывание**
   - Полный контроль над данными
   - Нет зависимости от внешних API
   - Работает без интернета (после загрузки модели)

2. **Качество поиска**
   - BGE-base-en-v1.5 показывает высокую точность (84.7% top-5)
   - Семантический поиск понимает синонимы и контекст
   - Лучше keyword-поиска для сложных запросов

3. **Масштабируемость**
   - ChromaDB эффективно обрабатывает миллионы векторов
   - Быстрый поиск (<100 мс для типичных запросов)
   - Поддержка инкрементальных обновлений

4. **Метаданные**
   - Сохранение источников для цитирования
   - Возможность фильтрации по источникам
   - Восстановление контекста документа

### Ограничения и улучшения

**Текущие ограничения:**
- Размер чанка фиксирован (1000 символов)
- Нет поддержки мультиязычности (только английский)
- Нет гибридного поиска (семантический + keyword)

**Возможные улучшения:**
- Адаптивный размер чанков на основе семантики
- Fine-tuning модели на доменных данных
- Добавление keyword-поиска для точных совпадений
- Поддержка фильтрации по метаданным в запросах
- Кэширование частых запросов

