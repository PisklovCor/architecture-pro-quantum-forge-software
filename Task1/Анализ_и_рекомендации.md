# Анализ моделей и инфраструктуры для RAG-бота QuantumForge Software

## Введение

Данный документ содержит результаты исследования и рекомендации по выбору технологического стека для создания RAG-бота для корпоративной базы знаний QuantumForge Software. Анализ проведен с учетом специфики компании: работа с конфиденциальными данными промышленных объектов, существующая инфраструктура AWS EKS, объем данных (18 000 Markdown/MDX файлов, 3 000 страниц Confluence, 250 PDF-спецификаций) и требования к конфиденциальности.

## 1. Сравнение LLM-моделей

### 1.1 Локальные модели Hugging Face

**Качество ответов:**
- Llama 3 70B демонстрирует конкурентоспособное качество с GPT-4: 70% точность на задачах классификации против 73% у GPT-4
- Преимущество на задачах генерации кода: на 15% выше точность в генерации Python-кода
- Уступает GPT-4 в математических рассуждениях (40% против 55%)
- Mistral 7B показывает отличные результаты для моделей меньшего размера, превосходя Llama 2 13B на большинстве бенчмарков

**Скорость работы:**
- Llama 3 70B на оптимизированных платформах: ~0.36 секунды за инференс против 0.54 секунды у GPT-4
- На платформе Groq: 309 токенов/сек против 36 токенов/сек у GPT-4 (примерно в 9 раз быстрее)
- RTX 5090 GPU: ~5,841 токенов/сек для Qwen2.5-Coder-7B
- Квантизация 4-bit и 8-bit дает ускорение в 2-4 раза при минимальной потере точности (~1%)

**Стоимость владения:**
- Локальное развертывание: капитальные затраты $833,806 для 8× NVIDIA H100 NVL 94GB
- Операционные расходы: $0.87/час на электроэнергию и охлаждение
- За 5 лет непрерывной работы: $871,912 против $4,306,416 в облаке (экономия $3,434,504)
- Точка окупаемости: ~8,900 часов использования (6-9 часов в день)

**Удобство развертывания:**
- Требует GPU-инфраструктуры, настройки inference-серверов (vLLM, llama.cpp, TGI)
- Необходима экспертиза в оптимизации батчинга, KV cache, квантовании
- Поддержка работы в air-gapped средах (критично для конфиденциальных данных)
- Полный контроль над данными и моделями

### 1.2 Облачные модели OpenAI / YandexGPT

**Качество ответов:**
- GPT-4: лидер в математических рассуждениях и общих рассуждениях (MMLU)
- GPT-4 Turbo: контекстное окно 128K токенов (как у Llama 3.1)
- YandexGPT Pro: оптимизирован для глубоких рассуждений, анализ документов, контекст до 32,000 токенов
- Ограниченная информация о бенчмарках YandexGPT в публичном доступе

**Скорость работы:**
- OpenAI API: 100-600 мс в зависимости от модели и нагрузки
- YandexGPT: производительность через облачную инфраструктуру (детали латентности доступны через enterprise-поддержку)
- Нет контроля над инфраструктурой и оптимизацией

**Стоимость владения:**
- GPT-5: $1.25 за миллион входных токенов, $10 за миллион выходных
- GPT-5 Pro: $15/$120 за миллион токенов
- YandexGPT Lite: $0.001667 за 1,000 входных токенов (синхронно)
- YandexGPT Pro: $0.010002 за 1,000 входных токенов
- Типичный RAG-запрос (1,500 входных + 500 выходных токенов): OpenAI ~$0.029-0.045, YandexGPT Lite ~$0.0033

**Удобство развертывания:**
- Минимальные усилия: настройка API-ключа и HTTP-интеграция
- Нет необходимости в GPU-инфраструктуре
- Зависимость от интернет-соединения и внешних сервисов
- Невозможность работы в air-gapped средах

### 1.3 Выводы по LLM-моделям

**Для QuantumForge Software:**
- Конфиденциальность данных промышленных объектов требует локального развертывания
- Существующая инфраструктура AWS EKS позволяет развернуть локальные модели в приватном кластере
- При объеме использования 6+ часов в день локальное развертывание экономически выгодно
- Рекомендация: **локальные модели Hugging Face (Llama 3.1 8B или Mistral 7B)** для баланса качества, скорости и ресурсов

## 2. Сравнение моделей эмбеддингов

### 2.1 Локальные Sentence-Transformers

**Скорость создания индекса:**
- На GPU (A10g/H100): 230,000 токенов/сек
- Для 1 млн документов (50 млн токенов): ~3.6 минуты на GPU
- Оптимизация Snowflake: 16× ускорение на H100 для коротких последовательностей, 4.2× для 512-токеновых
- Инкрементальные обновления: добавление 10,000 документов за ~22 секунды

**Качество поиска:**
- BGE-base-v1.5: 84.7% top-5 точность поиска (превосходит OpenAI text-embedding-3-small)
- all-MiniLM-L6-v2: 78.1% top-5 точность при минимальных ресурсах
- nomic-embed-text-v1: 86.2% top-5 точность
- Возможность доменной адаптации: улучшение на 10 процентных пунктов через fine-tuning

**Стоимость владения:**
- Локальная инфраструктура: A10g GPU ~$0.45/час в облаке или $10,000-15,000 покупка
- Для 50 млн токенов индексации: ~$0.11 в облаке (против $1,000 у OpenAI)
- Для 1 млн запросов/день: ~$10/год (против $36,500/год у OpenAI)
- При объеме >1-2 млрд токенов/год локальная инфраструктура экономически выгодна

**Удобство развертывания:**
- Требует GPU для production-нагрузок
- CPU-режим: 5-14 тыс. предложений/сек для легких моделей (достаточно для разработки)
- Квантованные модели (int8): 75% сокращение памяти, минимальная потеря точности

### 2.2 Облачные OpenAI Embeddings

**Скорость создания индекса:**
- Rate limits: 3,500 запросов/мин для text-embedding-3-small, 1,000/мин для large
- Для 100,000 документов (50 млн токенов): минимум 4 часа непрерывной работы API
- Batch API: 50% снижение стоимости, но до 24 часов задержки
- Невозможность real-time индексации из-за rate limits

**Качество поиска:**
- text-embedding-3-large: 64.6% NDCG на MTEB retrieval, 80.5% точность на реальных данных
- text-embedding-3-small: 62.3% NDCG, 75.8% точность
- Поддержка matryoshka dimensionality reduction для гибкости размерности

**Стоимость владения:**
- text-embedding-3-small: $0.02 за миллион токенов
- text-embedding-3-large: $0.13 за миллион токенов
- Batch API: 50% скидка
- Для 1 млн документов: $1,000 индексация + $36,500/год на запросы = ~$50,000/год

**Удобство развертывания:**
- Минимальная настройка: API-ключ и HTTP-вызовы
- Нет управления инфраструктурой
- Зависимость от сети и внешних сервисов

### 2.3 Выводы по моделям эмбеддингов

**Для QuantumForge Software:**
- Объем данных (18,000 + 3,000 + 250 документов) и рост ~400 страниц/месяц требуют эффективной индексации
- Real-time обновления критичны для актуальности базы знаний
- Конфиденциальность данных исключает передачу в облако
- Рекомендация: **локальные Sentence-Transformers (BGE-base-en-v1.5 или nomic-embed-text-v1)** с GPU-ускорением

## 3. Сравнение векторных баз данных

### 3.1 ChromaDB

**Скорость поиска и индексации:**
- Низкая конкурентность: субсекундные ответы
- При 100 конкурентных запросах: среднее время ответа 23.08 секунд (последовательная обработка)
- Индексация: адекватная для типичных RAG-сценариев, но уступает оптимизированному FAISS
- Использует HNSW (Hierarchical Navigable Small World) как алгоритм по умолчанию

**Сложность внедрения и поддержки:**
- Простая установка: `pip install chromadb`
- Встроенная персистентность через SQLite (v0.4)
- Thread-safe для локальных развертываний
- Минимальная конфигурация: разумные значения по умолчанию
- Поддержка метаданных и фильтрации из коробки

**Удобство в работе:**
- Интуитивный Python API, похожий на работу с базами данных
- Интеграция с LangChain, LlamaIndex, Haystack
- Поддержка метаданных для фильтрации (MongoDB-подобный синтаксис)
- Множественные режимы: EphemeralClient, PersistentClient, HttpClient

**Стоимость владения:**
- Локальное развертывание: минимальные требования (4GB RAM, 2-4 CPU cores для миллионов документов)
- ChromaDB Cloud: $2.5 за GiB записанных данных, $0.0075 за TiB просканированных
- Операционная простота снижает DevOps-нагрузку

### 3.2 FAISS

**Скорость поиска и индексации:**
- При 100 конкурентных запросах: среднее время ответа 9.81 секунд
- GPU-ускорение: 3-10× ускорение по сравнению с CPU
- Индексация на GPU: может обрабатывать сотни миллионов векторов на одной GPU
- Поддержка различных индексов: Flat, IVF, Product Quantization, HNSW

**Сложность внедрения и поддержки:**
- Требует выбора типа индекса из десятков вариантов
- Необходима настройка параметров (nprobe, кластеры, квантование)
- Явное управление персистентностью (save_local/load_index_from_file)
- Не поддерживает real-time обновления индекса (требуется пересборка)
- Нет встроенной поддержки метаданных (требуется внешнее хранилище)

**Удобство в работе:**
- Низкоуровневый API, требует понимания алгоритмов векторного поиска
- Интеграция с RAG-фреймворками требует больше конфигурации
- Необходима экспертиза в настройке для production

**Стоимость владения:**
- Требует GPU для оптимальной производительности ($1,000-5,000 за GPU)
- Масштабирование на одном узле до миллиардов векторов
- Требуется дополнительная инфраструктура для персистентности и метаданных
- Высокие инженерные затраты на поддержку

### 3.3 Выводы по векторным БД

**Для QuantumForge Software:**
- Объем данных: ~21,250 документов (не миллиарды) - ChromaDB достаточен
- Требования к конфиденциальности: локальное развертывание
- Необходимость метаданных для фильтрации по источникам и доступам
- Простота поддержки важна для команды
- Рекомендация: **ChromaDB** для баланса функциональности, простоты и производительности

## 4. Рекомендации по конфигурации сервера

### 4.1 Анализ требований

**Объем данных:**
- ~18,000 Markdown/MDX файлов: ~150-300 МБ
- ~3,000 страниц Confluence: ~150-200 МБ
- ~250 PDF: ~500 МБ - 1 ГБ
- Итого исходных данных: ~3-5 ГБ
- Векторные индексы: 6-15 ГБ (в зависимости от модели эмбеддингов)

**Требования к производительности:**
- Индексация: единоразово при развертывании, затем инкрементальные обновления
- Запросы: поддержка множественных одновременных пользователей
- Латентность: <300 мс для интерактивных запросов

**Требования к конфиденциальности:**
- Развертывание в приватном AWS EKS кластере
- Данные не должны покидать инфраструктуру компании
- Поддержка VPC isolation

### 4.2 Рекомендуемая конфигурация

**Вариант 1: Оптимальная конфигурация для production**

| Компонент | Требование | Обоснование |
|-----------|-----------|-------------|
| CPU | 8-16 ядер (2.5+ GHz) | Параллельная обработка документов при индексации и запросах |
| RAM | 48-64 ГБ | 16 ГБ для LLM модели + 16 ГБ для эмбеддинг-модели + 16-32 ГБ для буферизации |
| GPU | NVIDIA A100 40GB или RTX 6000 24GB | Ускорение генерации эмбеддингов (10-20× быстрее CPU) и LLM inference |
| Хранилище | 500 ГБ - 1 ТБ SSD (NVMe) | Векторные индексы + исходные данные + модели + логи |
| ОС | Ubuntu 24.04 LTS | Поддержка Ollama и основных фреймворков |

**Для AWS EKS:**
- Worker Nodes: `c6i.4xlarge` (16 vCPU, 32 GiB RAM) - минимум 3 узла
- GPU Nodes: `g4dn.12xlarge` (48 vCPU, 192 GiB RAM, 4× NVIDIA T4) или `p3.8xlarge` (32 vCPU, 244 GiB RAM, 8× V100)
- Persistent Volume: EBS 500 ГБ - 1 ТБ для индексов

**Вариант 2: Экономичная конфигурация**

| Компонент | Требование | Обоснование |
|-----------|-----------|-------------|
| CPU | 16 ядер | Достаточно для обработки запросов |
| RAM | 32 ГБ | Минимум для работы моделей |
| GPU | Опционально (NVIDIA T4) | Ускорение при наличии, но не обязательно |
| Хранилище | 500 ГБ SSD | Базовые требования |

**Для AWS EKS:**
- Worker Nodes: `c6i.2xlarge` (8 vCPU, 16 GiB RAM) - 2-3 узла
- Без GPU nodes (CPU-only inference с квантованными моделями)

## 5. Итоговые рекомендации: варианты конфигурации

### Вариант 1: Полностью локальное развертывание (рекомендуемый)

**Компоненты:**
- **LLM:** Llama 3.1 8B (локально, квантованная 4-bit)
- **Эмбеддинги:** BGE-base-en-v1.5 (локально, Sentence-Transformers)
- **Векторная БД:** ChromaDB (локально, PersistentClient)
- **Инфраструктура:** AWS EKS приватный кластер, GPU nodes (g4dn.12xlarge)

**Преимущества:**
- Полный контроль над данными и конфиденциальность
- Экономия $3+ млн за 5 лет при интенсивном использовании
- Real-time обновления индексов без rate limits
- Возможность доменной адаптации моделей
- Работа в air-gapped средах

**Недостатки:**
- Требует GPU-инфраструктуры и экспертизы
- Высокие начальные капитальные затраты
- Необходимость поддержки инфраструктуры

**Стоимость (AWS EKS):**
- GPU nodes: ~$2,000-3,000/месяц
- CPU nodes: ~$500-800/месяц
- Storage: ~$100-200/месяц
- **Итого: ~$2,600-4,000/месяц**

**Применимость:** Оптимален для QuantumForge Software из-за требований конфиденциальности и существующей AWS-инфраструктуры.

### Вариант 2: Гибридный подход

**Компоненты:**
- **LLM:** YandexGPT Pro (облако) для запросов, локальная модель для конфиденциальных данных
- **Эмбеддинги:** Локальные Sentence-Transformers для индексации, OpenAI Batch API для периодических обновлений
- **Векторная БД:** ChromaDB (локально)
- **Инфраструктура:** AWS EKS, CPU nodes + облачные API

**Преимущества:**
- Снижение затрат на инфраструктуру (нет GPU nodes)
- Использование лучших облачных моделей для общих запросов
- Локальная обработка конфиденциальных данных
- Гибкость в масштабировании

**Недостатки:**
- Зависимость от внешних API для части запросов
- Сложность маршрутизации запросов (конфиденциальные vs общие)
- Потенциальные проблемы с конфиденциальностью при использовании облака

**Стоимость:**
- CPU nodes: ~$500-800/месяц
- YandexGPT API: ~$500-1,500/месяц (зависит от объема)
- Storage: ~$100-200/месяц
- **Итого: ~$1,100-2,500/месяц**

**Применимость:** Подходит если можно четко разделить конфиденциальные и общие данные.

### Вариант 3: Облачное развертывание (не рекомендуется для данного кейса)

**Компоненты:**
- **LLM:** YandexGPT Pro или OpenAI GPT-4
- **Эмбеддинги:** OpenAI text-embedding-3-small
- **Векторная БД:** Pinecone или ChromaDB Cloud
- **Инфраструктура:** Полностью облачная

**Преимущества:**
- Минимальные начальные затраты
- Простота развертывания и поддержки
- Автоматическое масштабирование

**Недостатки:**
- Нарушение требований конфиденциальности (данные передаются в облако)
- Высокие операционные расходы при масштабировании (~$50,000+/год)
- Невозможность работы с конфиденциальными данными
- Зависимость от внешних сервисов

**Стоимость:**
- LLM API: ~$2,000-5,000/месяц
- Embeddings API: ~$3,000-4,000/месяц
- Vector DB: ~$500-1,000/месяц
- **Итого: ~$5,500-10,000/месяц**

**Применимость:** Не подходит из-за требований конфиденциальности.

### Вариант 4: Минималистичное локальное развертывание

**Компоненты:**
- **LLM:** Mistral 7B (локально, CPU-only с квантованием)
- **Эмбеддинги:** all-MiniLM-L6-v2 (локально, CPU)
- **Векторная БД:** ChromaDB (локально)
- **Инфраструктура:** AWS EKS, только CPU nodes

**Преимущества:**
- Минимальные затраты на инфраструктуру
- Полная конфиденциальность
- Простота развертывания

**Недостатки:**
- Медленная обработка запросов (CPU-only)
- Ограниченное качество ответов (меньшие модели)
- Не подходит для production с высокой нагрузкой

**Стоимость:**
- CPU nodes: ~$500-800/месяц
- Storage: ~$100-200/месяц
- **Итого: ~$600-1,000/месяц**

**Применимость:** Только для пилотных проектов или очень ограниченного бюджета.

## 6. Рекомендуемое решение

### Выбор: Вариант 1 - Полностью локальное развертывание

**Обоснование выбора:**

1. **Требования конфиденциальности:**
   - QuantumForge Software работает с конфиденциальными данными промышленных объектов
   - Клиенты (ABB, Wärtsilä) требуют гарантий безопасности данных
   - Локальное развертывание - единственный способ гарантировать, что данные не покидают инфраструктуру компании

2. **Экономическая эффективность:**
   - При использовании 6+ часов в день локальное развертывание окупается за 1-2 года
   - Долгосрочная экономия $3+ млн за 5 лет
   - Существующая AWS-инфраструктура снижает затраты на миграцию

3. **Технические требования:**
   - Объем данных (~21,250 документов) хорошо укладывается в возможности локального развертывания
   - Real-time обновления критичны для актуальности базы знаний
   - ChromaDB обеспечивает необходимую функциональность без избыточной сложности

4. **Интеграция с существующей инфраструктурой:**
   - AWS EKS уже используется в продакшене
   - Приватный кластер с VPC isolation решает вопросы безопасности
   - Возможность использования существующих DevOps-процессов

### Детальная конфигурация

**LLM:**
- **Модель:** Llama 3.1 8B (квантованная 4-bit через llama.cpp)
- **Развертывание:** Ollama в Kubernetes pod с GPU
- **Ресурсы:** 16 ГБ RAM, 1× NVIDIA T4 GPU (минимум) или A10g (оптимально)

**Эмбеддинги:**
- **Модель:** BGE-base-en-v1.5 (Sentence-Transformers)
- **Развертывание:** Отдельный сервис в Kubernetes с GPU
- **Ресурсы:** 8 ГБ RAM, 1× NVIDIA T4 GPU

**Векторная БД:**
- **Решение:** ChromaDB (PersistentClient)
- **Развертывание:** Kubernetes StatefulSet с PersistentVolume
- **Ресурсы:** 8 ГБ RAM, 2 CPU cores, 500 ГБ EBS storage

**Инфраструктура AWS EKS:**
- **GPU Worker Nodes:** 1-2× g4dn.12xlarge для LLM и эмбеддингов
- **CPU Worker Nodes:** 2-3× c6i.2xlarge для ChromaDB и оркестрации
- **Storage:** EBS volumes для персистентности (500 ГБ - 1 ТБ)
- **Network:** Приватный VPC, VPC endpoints для AWS сервисов

### План внедрения

**Фаза 1: Пилот (1-2 месяца)**
- Развертывание минимальной конфигурации на CPU nodes
- Тестирование на подмножестве документов (1,000-2,000)
- Оценка качества ответов и производительности

**Фаза 2: Производство (2-3 месяца)**
- Добавление GPU nodes
- Полная индексация всех документов
- Интеграция с существующими системами (Slack, Confluence)
- Настройка мониторинга и алертинга

**Фаза 3: Оптимизация (ongoing)**
- Fine-tuning моделей на доменных данных
- Оптимизация chunking-стратегии
- Настройка гибридного поиска (семантический + keyword)
- Масштабирование при росте нагрузки

### Ожидаемые результаты

**Производительность:**
- Время ответа: <300 мс для типичных запросов
- Поддержка 50-100 одновременных пользователей
- Индексация всех документов: 2-4 часа (единоразово)
- Инкрементальные обновления: <5 минут для 400 новых страниц/месяц

**Качество:**
- Точность поиска: >85% top-5 recall
- Качество ответов: сопоставимо с GPT-4 для доменных запросов
- Снижение времени поиска информации: с 4 часов/неделю до <5 минут

**Экономика:**
- Операционные расходы: ~$2,600-4,000/месяц
- Экономия времени сотрудников: ~$50,000-100,000/год (оценка)
- ROI: окупаемость за 12-18 месяцев

## 7. Заключение

Для QuantumForge Software оптимальным решением является **полностью локальное развертывание RAG-бота** с использованием:
- **LLM:** Llama 3.1 8B (локально, квантованная)
- **Эмбеддинги:** BGE-base-en-v1.5 (локально)
- **Векторная БД:** ChromaDB (локально)

Это решение обеспечивает:
1. Полную конфиденциальность данных (критично для промышленных объектов)
2. Экономическую эффективность при долгосрочном использовании
3. Интеграцию с существующей AWS EKS инфраструктурой
4. Возможность real-time обновлений базы знаний
5. Масштабируемость для роста компании

Альтернативные варианты (гибридный или полностью облачный) не подходят из-за требований конфиденциальности, хотя могут быть рассмотрены для некритичных данных в будущем.

